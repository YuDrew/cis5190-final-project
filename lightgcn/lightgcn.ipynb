{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import random \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import *\n",
    "from scipy.spatial import distance\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "np.random.seed(42)  # don't change this line\n",
    "\n",
    "import base64\n",
    "import datetime\n",
    "!pip install recommenders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.9.16 (main, Mar  8 2023, 04:29:44) \n",
      "[Clang 14.0.6 ]\n",
      "Pandas version: 1.5.3\n",
      "Tensorflow version: 2.10.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import scrapbook as sb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR') # only show error messages\n",
    "\n",
    "from recommenders.utils.timer import Timer\n",
    "from recommenders.models.deeprec.models.graphrec.lightgcn import LightGCN\n",
    "from recommenders.models.deeprec.DataModel.ImplicitCF import ImplicitCF\n",
    "from recommenders.datasets import movielens\n",
    "from recommenders.datasets.python_splitters import python_stratified_split\n",
    "from recommenders.evaluation.python_evaluation import map_at_k, ndcg_at_k, precision_at_k, recall_at_k\n",
    "from recommenders.utils.constants import SEED as DEFAULT_SEED\n",
    "from recommenders.models.deeprec.deeprec_utils import prepare_hparams\n",
    "\n",
    "DETAULT_SEED = 42 # this is what is is lol\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Pandas version: {}\".format(pd.__version__))\n",
    "print(\"Tensorflow version: {}\".format(tf.__version__))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions for Data Cleanup\n",
    "- `save_and_compress_embeddings(file_name, first_col_name)`: put file of embedding output and the name of the first col (user / item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_and_compress_embeddings(file_name, first_col_name):\n",
    "    try:\n",
    "        df = pd.read_csv(file_name, header = None, delimiter='\\t')\n",
    "        df.columns = [first_col_name, 'embeddings']\n",
    "        embeddings = df.embeddings.str.split(' ', expand=True).add_prefix('embedding_')\n",
    "        df = pd.concat([df[first_col_name], embeddings], axis=1)\n",
    "        df.to_csv(f'{file_name}.bz2', compression='bz2', index=False)\n",
    "        os.remove(file_name)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(f'Failed to compress {file_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a yaml file name in the structure of 'embed_size_n_layers_batch_size_decay_epochs_learning_rate_eval_epoch_top_k', return the hyperparameters\n",
    "def get_hyperparameters(yaml_file):\n",
    "    embed_size = int(yaml_file.split('_')[0])\n",
    "    n_layers = int(yaml_file.split('_')[1])\n",
    "    batch_size = int(yaml_file.split('_')[2])\n",
    "    # decay has a decimal that we changed to an underscore, so we need to change it back\n",
    "    decay = float(yaml_file.split('_')[3] + '.' + yaml_file.split('_')[4])\n",
    "    epochs = int(yaml_file.split('_')[5])\n",
    "    # learning rate has a decimal that we changed to an underscore, so we need to change it back\n",
    "    learning_rate = float(yaml_file.split('_')[6] + '.' + yaml_file.split('_')[7])\n",
    "    eval_epoch = int(yaml_file.split('_')[8])\n",
    "    top_k = int(yaml_file.split('_')[9])\n",
    "    return embed_size, n_layers, batch_size, decay, epochs, learning_rate, eval_epoch, top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a drop duplicates function that keeps the row with the highest MAP\n",
    "def drop_duplicates(df):\n",
    "    df.sort_values(by=['MAP'], ascending=False, inplace=True)\n",
    "    df.drop_duplicates(subset=['embed_size', 'n_layers', 'batch_size', 'decay', 'epochs', 'learning_rate', 'eval_epoch', 'top_k'], keep='first', inplace=True)\n",
    "    df.reset_index(inplace=True)\n",
    "    df.drop(columns=['index'], inplace=True)\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load amazon data into `amazon_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>productId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AKM1MP6P0OYPR</td>\n",
       "      <td>0132793040</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1365811200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A2CX7LUOHB2NDG</td>\n",
       "      <td>0321732944</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1341100800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A2NWSAGRHCP8N5</td>\n",
       "      <td>0439886341</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1367193600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A2WNBOD3WNDNKT</td>\n",
       "      <td>0439886341</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1374451200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A1GI0U4ZRJA8WN</td>\n",
       "      <td>0439886341</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1334707200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7824477</th>\n",
       "      <td>A2YZI3C9MOHC0L</td>\n",
       "      <td>BT008UKTMW</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1396569600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7824478</th>\n",
       "      <td>A322MDK0M89RHN</td>\n",
       "      <td>BT008UKTMW</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1313366400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7824479</th>\n",
       "      <td>A1MH90R0ADMIK0</td>\n",
       "      <td>BT008UKTMW</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1404172800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7824480</th>\n",
       "      <td>A10M2KEFPEQDHN</td>\n",
       "      <td>BT008UKTMW</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1297555200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7824481</th>\n",
       "      <td>A2G81TMIOIDEQQ</td>\n",
       "      <td>BT008V9J9U</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1312675200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7824482 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 userId   productId  rating   timestamp\n",
       "0         AKM1MP6P0OYPR  0132793040     5.0  1365811200\n",
       "1        A2CX7LUOHB2NDG  0321732944     5.0  1341100800\n",
       "2        A2NWSAGRHCP8N5  0439886341     1.0  1367193600\n",
       "3        A2WNBOD3WNDNKT  0439886341     3.0  1374451200\n",
       "4        A1GI0U4ZRJA8WN  0439886341     1.0  1334707200\n",
       "...                 ...         ...     ...         ...\n",
       "7824477  A2YZI3C9MOHC0L  BT008UKTMW     5.0  1396569600\n",
       "7824478  A322MDK0M89RHN  BT008UKTMW     5.0  1313366400\n",
       "7824479  A1MH90R0ADMIK0  BT008UKTMW     4.0  1404172800\n",
       "7824480  A10M2KEFPEQDHN  BT008UKTMW     4.0  1297555200\n",
       "7824481  A2G81TMIOIDEQQ  BT008V9J9U     5.0  1312675200\n",
       "\n",
       "[7824482 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"https://5190-hav-recommendation-data.s3.us-east-1.amazonaws.com/ratings_Electronics.csv\"\n",
    "amazon_data = pd.read_csv(path, header=None)\n",
    "amazon_data.columns = ['userId', 'productId', 'rating', 'timestamp']\n",
    "amazon_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing \n",
    "- subset to only users with 50+ ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125871, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>productId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>user_rating_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>A3BY5KCNQZXV5U</td>\n",
       "      <td>0594451647</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1390176000</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14863</th>\n",
       "      <td>A3BY5KCNQZXV5U</td>\n",
       "      <td>B00000JD4V</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1118016000</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134213</th>\n",
       "      <td>A3BY5KCNQZXV5U</td>\n",
       "      <td>B000063574</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1016668800</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338368</th>\n",
       "      <td>A3BY5KCNQZXV5U</td>\n",
       "      <td>B0000CDJP8</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1258761600</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634048</th>\n",
       "      <td>A3BY5KCNQZXV5U</td>\n",
       "      <td>B0007Y794O</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1369872000</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                userId   productId  rating   timestamp  user_rating_count\n",
       "94      A3BY5KCNQZXV5U  0594451647     5.0  1390176000                 50\n",
       "14863   A3BY5KCNQZXV5U  B00000JD4V     4.0  1118016000                 50\n",
       "134213  A3BY5KCNQZXV5U  B000063574     5.0  1016668800                 50\n",
       "338368  A3BY5KCNQZXV5U  B0000CDJP8     5.0  1258761600                 50\n",
       "634048  A3BY5KCNQZXV5U  B0007Y794O     5.0  1369872000                 50"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing\n",
    "user_rating_count = amazon_data['userId'].value_counts().rename('user_rating_count')\n",
    "augmented_amazon_data = amazon_data.merge(user_rating_count.to_frame(), left_on='userId', right_index=True)\n",
    "subset_df = augmented_amazon_data[augmented_amazon_data.user_rating_count >= 50]\n",
    "print(subset_df.shape)\n",
    "subset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>productId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>user_rating_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7811895</th>\n",
       "      <td>A328S9RN3U5M68</td>\n",
       "      <td>B00JGL37FO</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1400976000</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7817686</th>\n",
       "      <td>A328S9RN3U5M68</td>\n",
       "      <td>B00K00FN3O</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1400544000</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7824063</th>\n",
       "      <td>A328S9RN3U5M68</td>\n",
       "      <td>B00L21HC7A</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1405123200</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7824081</th>\n",
       "      <td>A328S9RN3U5M68</td>\n",
       "      <td>B00L2442H0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1405123200</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7824103</th>\n",
       "      <td>A328S9RN3U5M68</td>\n",
       "      <td>B00L26YDA4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1405123200</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 userId   productId  rating   timestamp  user_rating_count\n",
       "7811895  A328S9RN3U5M68  B00JGL37FO     5.0  1400976000                 76\n",
       "7817686  A328S9RN3U5M68  B00K00FN3O     5.0  1400544000                 76\n",
       "7824063  A328S9RN3U5M68  B00L21HC7A     5.0  1405123200                 76\n",
       "7824081  A328S9RN3U5M68  B00L2442H0     5.0  1405123200                 76\n",
       "7824103  A328S9RN3U5M68  B00L26YDA4     5.0  1405123200                 76"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_df.tail()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 4: LightGCN - Run on Our Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top k items to recommend\n",
    "TOP_K = 10\n",
    "\n",
    "# Select MovieLens data size: 100k, 1m, 10m, or 20m\n",
    "AMAZON_DATA_SIZE = 100000\n",
    "\n",
    "# Model parameters\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "SEED = DEFAULT_SEED  # Set None for non-deterministic results\n",
    "\n",
    "yaml_file = \"lightgcn.yaml\"\n",
    "user_file = \"embeddings/amazon/100k_user_embeddings.csv\"\n",
    "item_file = \"embeddings/amazon/100k_item_embeddings.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>itemID</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>A3BY5KCNQZXV5U</td>\n",
       "      <td>0594451647</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1390176000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14863</th>\n",
       "      <td>A3BY5KCNQZXV5U</td>\n",
       "      <td>B00000JD4V</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1118016000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134213</th>\n",
       "      <td>A3BY5KCNQZXV5U</td>\n",
       "      <td>B000063574</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1016668800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338368</th>\n",
       "      <td>A3BY5KCNQZXV5U</td>\n",
       "      <td>B0000CDJP8</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1258761600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634048</th>\n",
       "      <td>A3BY5KCNQZXV5U</td>\n",
       "      <td>B0007Y794O</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1369872000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                userID      itemID  rating   timestamp\n",
       "94      A3BY5KCNQZXV5U  0594451647     5.0  1390176000\n",
       "14863   A3BY5KCNQZXV5U  B00000JD4V     4.0  1118016000\n",
       "134213  A3BY5KCNQZXV5U  B000063574     5.0  1016668800\n",
       "338368  A3BY5KCNQZXV5U  B0000CDJP8     5.0  1258761600\n",
       "634048  A3BY5KCNQZXV5U  B0007Y794O     5.0  1369872000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = subset_df[:AMAZON_DATA_SIZE]\n",
    "df = df.rename(columns={'userId': 'userID', 'productId': 'itemID'})\n",
    "df.drop(columns='user_rating_count', inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = python_stratified_split(df, ratio=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yudrew/Dev/cis5190/.conda/lib/python3.9/site-packages/recommenders/models/deeprec/DataModel/ImplicitCF.py:73: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = train if test is None else train.append(test)\n"
     ]
    }
   ],
   "source": [
    "data = ImplicitCF(train=train, test=test, seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = prepare_hparams(\n",
    "    yaml_file,\n",
    "    n_layers=3,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    learning_rate=0.005,\n",
    "    eval_epoch=5,\n",
    "    top_k=TOP_K,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already create adjacency matrix.\n",
      "Already normalize adjacency matrix.\n",
      "Using xavier initialization.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-25 22:06:28.158233: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-25 22:06:28.163749: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n"
     ]
    }
   ],
   "source": [
    "model = LightGCN(hparams, data, seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 (train)4.1s: train loss = 0.51765 = (mf)0.51751 + (embed)0.00014\n",
      "Epoch 2 (train)3.9s: train loss = 0.16956 = (mf)0.16891 + (embed)0.00065\n",
      "Epoch 3 (train)3.9s: train loss = 0.09665 = (mf)0.09566 + (embed)0.00099\n",
      "Epoch 4 (train)3.9s: train loss = 0.06747 = (mf)0.06625 + (embed)0.00122\n",
      "Epoch 5 (train)3.9s + (eval)0.9s: train loss = 0.05316 = (mf)0.05176 + (embed)0.00140, recall = 0.01625, ndcg = 0.03706, precision = 0.03427, map = 0.00614\n",
      "Epoch 6 (train)3.9s: train loss = 0.04242 = (mf)0.04087 + (embed)0.00155\n",
      "Epoch 7 (train)3.9s: train loss = 0.03598 = (mf)0.03429 + (embed)0.00168\n",
      "Epoch 8 (train)3.9s: train loss = 0.03176 = (mf)0.02995 + (embed)0.00180\n",
      "Epoch 9 (train)4.1s: train loss = 0.02699 = (mf)0.02508 + (embed)0.00191\n",
      "Epoch 10 (train)4.0s + (eval)0.8s: train loss = 0.02328 = (mf)0.02127 + (embed)0.00201, recall = 0.01567, ndcg = 0.03601, precision = 0.03333, map = 0.00585\n",
      "Epoch 11 (train)3.9s: train loss = 0.02062 = (mf)0.01852 + (embed)0.00210\n",
      "Epoch 12 (train)3.9s: train loss = 0.01817 = (mf)0.01599 + (embed)0.00218\n",
      "Epoch 13 (train)3.9s: train loss = 0.01724 = (mf)0.01497 + (embed)0.00226\n",
      "Epoch 14 (train)3.9s: train loss = 0.01573 = (mf)0.01340 + (embed)0.00233\n",
      "Epoch 15 (train)3.9s + (eval)0.8s: train loss = 0.01406 = (mf)0.01167 + (embed)0.00240, recall = 0.01557, ndcg = 0.03693, precision = 0.03359, map = 0.00594\n",
      "Epoch 16 (train)4.1s: train loss = 0.01361 = (mf)0.01115 + (embed)0.00246\n",
      "Epoch 17 (train)4.0s: train loss = 0.01271 = (mf)0.01019 + (embed)0.00252\n",
      "Epoch 18 (train)4.1s: train loss = 0.01141 = (mf)0.00884 + (embed)0.00257\n",
      "Epoch 19 (train)4.0s: train loss = 0.01091 = (mf)0.00829 + (embed)0.00262\n",
      "Epoch 20 (train)4.1s + (eval)0.9s: train loss = 0.01040 = (mf)0.00774 + (embed)0.00266, recall = 0.01530, ndcg = 0.03551, precision = 0.03274, map = 0.00573\n",
      "Epoch 21 (train)4.1s: train loss = 0.01025 = (mf)0.00754 + (embed)0.00271\n",
      "Epoch 22 (train)4.1s: train loss = 0.00982 = (mf)0.00707 + (embed)0.00275\n",
      "Epoch 23 (train)4.1s: train loss = 0.00905 = (mf)0.00626 + (embed)0.00278\n",
      "Epoch 24 (train)4.0s: train loss = 0.00875 = (mf)0.00592 + (embed)0.00282\n",
      "Epoch 25 (train)4.1s + (eval)0.8s: train loss = 0.00844 = (mf)0.00558 + (embed)0.00286, recall = 0.01442, ndcg = 0.03438, precision = 0.03121, map = 0.00560\n",
      "Epoch 26 (train)4.1s: train loss = 0.00836 = (mf)0.00548 + (embed)0.00289\n",
      "Epoch 27 (train)4.1s: train loss = 0.00835 = (mf)0.00543 + (embed)0.00291\n",
      "Epoch 28 (train)4.0s: train loss = 0.00776 = (mf)0.00483 + (embed)0.00294\n",
      "Epoch 29 (train)4.2s: train loss = 0.00776 = (mf)0.00480 + (embed)0.00296\n",
      "Epoch 30 (train)4.2s + (eval)0.9s: train loss = 0.00728 = (mf)0.00430 + (embed)0.00298, recall = 0.01357, ndcg = 0.03177, precision = 0.02942, map = 0.00500\n",
      "Epoch 31 (train)4.0s: train loss = 0.00754 = (mf)0.00453 + (embed)0.00300\n",
      "Epoch 32 (train)4.0s: train loss = 0.00738 = (mf)0.00436 + (embed)0.00302\n",
      "Epoch 33 (train)4.0s: train loss = 0.00689 = (mf)0.00385 + (embed)0.00304\n",
      "Epoch 34 (train)4.0s: train loss = 0.00694 = (mf)0.00389 + (embed)0.00305\n",
      "Epoch 35 (train)4.1s + (eval)0.8s: train loss = 0.00690 = (mf)0.00383 + (embed)0.00307, recall = 0.01278, ndcg = 0.03067, precision = 0.02789, map = 0.00480\n",
      "Epoch 36 (train)4.0s: train loss = 0.00656 = (mf)0.00348 + (embed)0.00308\n",
      "Epoch 37 (train)4.0s: train loss = 0.00629 = (mf)0.00319 + (embed)0.00309\n",
      "Epoch 38 (train)4.0s: train loss = 0.00630 = (mf)0.00319 + (embed)0.00310\n",
      "Epoch 39 (train)4.0s: train loss = 0.00610 = (mf)0.00299 + (embed)0.00311\n",
      "Epoch 40 (train)4.1s + (eval)0.9s: train loss = 0.00599 = (mf)0.00288 + (embed)0.00311, recall = 0.01268, ndcg = 0.02984, precision = 0.02730, map = 0.00473\n",
      "Epoch 41 (train)4.0s: train loss = 0.00588 = (mf)0.00275 + (embed)0.00312\n",
      "Epoch 42 (train)4.0s: train loss = 0.00598 = (mf)0.00286 + (embed)0.00312\n",
      "Epoch 43 (train)4.1s: train loss = 0.00609 = (mf)0.00296 + (embed)0.00313\n",
      "Epoch 44 (train)4.2s: train loss = 0.00576 = (mf)0.00263 + (embed)0.00313\n",
      "Epoch 45 (train)4.0s + (eval)0.8s: train loss = 0.00553 = (mf)0.00240 + (embed)0.00313, recall = 0.01151, ndcg = 0.02688, precision = 0.02526, map = 0.00421\n",
      "Epoch 46 (train)4.0s: train loss = 0.00548 = (mf)0.00235 + (embed)0.00314\n",
      "Epoch 47 (train)4.1s: train loss = 0.00543 = (mf)0.00230 + (embed)0.00313\n",
      "Epoch 48 (train)4.1s: train loss = 0.00561 = (mf)0.00248 + (embed)0.00313\n",
      "Epoch 49 (train)4.1s: train loss = 0.00549 = (mf)0.00236 + (embed)0.00312\n",
      "Epoch 50 (train)4.4s + (eval)0.9s: train loss = 0.00516 = (mf)0.00204 + (embed)0.00312, recall = 0.01245, ndcg = 0.02807, precision = 0.02696, map = 0.00439\n",
      "Took 210.13000470800011 seconds for training.\n"
     ]
    }
   ],
   "source": [
    "with Timer() as train_time:\n",
    "    model.fit()\n",
    "\n",
    "print(\"Took {} seconds for training.\".format(train_time.interval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>itemID</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A100UD67AHFODS</td>\n",
       "      <td>B001FA1NK0</td>\n",
       "      <td>12.427428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A100UD67AHFODS</td>\n",
       "      <td>B002V88HFE</td>\n",
       "      <td>10.693367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A100UD67AHFODS</td>\n",
       "      <td>B005CLPP84</td>\n",
       "      <td>10.614769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A100UD67AHFODS</td>\n",
       "      <td>B001TH7GUU</td>\n",
       "      <td>10.468667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A100UD67AHFODS</td>\n",
       "      <td>B0019EHU8G</td>\n",
       "      <td>10.422582</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           userID      itemID  prediction\n",
       "0  A100UD67AHFODS  B001FA1NK0   12.427428\n",
       "1  A100UD67AHFODS  B002V88HFE   10.693367\n",
       "2  A100UD67AHFODS  B005CLPP84   10.614769\n",
       "3  A100UD67AHFODS  B001TH7GUU   10.468667\n",
       "4  A100UD67AHFODS  B0019EHU8G   10.422582"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk_scores = model.recommend_k_items(test, top_k=TOP_K, remove_seen=True)\n",
    "\n",
    "topk_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP:\t0.004391\n",
      "NDCG:\t0.028074\n",
      "Precision@K:\t0.026956\n",
      "Recall@K:\t0.012454\n"
     ]
    }
   ],
   "source": [
    "eval_map = map_at_k(test, topk_scores, k=TOP_K)\n",
    "eval_ndcg = ndcg_at_k(test, topk_scores, k=TOP_K)\n",
    "eval_precision = precision_at_k(test, topk_scores, k=TOP_K)\n",
    "eval_recall = recall_at_k(test, topk_scores, k=TOP_K)\n",
    "\n",
    "print(\"MAP:\\t%f\" % eval_map,\n",
    "      \"NDCG:\\t%f\" % eval_ndcg,\n",
    "      \"Precision@K:\\t%f\" % eval_precision,\n",
    "      \"Recall@K:\\t%f\" % eval_recall, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.infer_embedding(user_file, item_file)\n",
    "save_and_compress_embeddings(user_file, \"userID\")\n",
    "save_and_compress_embeddings(item_file, \"itemID\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basically Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['64_7_2048_0_01_1000_0_001_-1_20',\n",
       " '64_7_2048_0_01_1000_0_01_-1_20',\n",
       " '64_7_2048_0_1_1000_0_001_-1_20',\n",
       " '64_7_2048_0_1_1000_0_01_-1_20']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yaml \n",
    "# \n",
    "yaml_dir = 'yamls'\n",
    "yaml_files = []\n",
    "\n",
    "# read in results to see which yaml files have already been written\n",
    "old_results_df = pd.read_csv('lightgcn_results.csv')\n",
    "written_yaml_files = old_results_df['yaml_file'].tolist()\n",
    "\n",
    "# write a bunch of different combinations of yaml files\n",
    "embed_size_list = [64]\n",
    "n_layers_list = [7]\n",
    "batch_size_list = [2048]\n",
    "decay_list = [0.01, 0.1]\n",
    "epochs_list = [1000]\n",
    "learning_rate_list = [0.001,0.01]\n",
    "eval_epoch_list = [-1]\n",
    "top_k_list = [20]\n",
    "\n",
    "# create a for loop that writes the yaml files for all combinations of the above hyperparameters\n",
    "for embed_size in embed_size_list:\n",
    "    for n_layers in n_layers_list:\n",
    "        for batch_size in batch_size_list:\n",
    "            for decay in decay_list:\n",
    "                for epochs in epochs_list:\n",
    "                    for learning_rate in learning_rate_list:\n",
    "                        for eval_epoch in eval_epoch_list:\n",
    "                            for top_k in top_k_list:\n",
    "                                filename = f'{embed_size}_{n_layers}_{batch_size}_{decay}_{epochs}_{learning_rate}_{eval_epoch}_{top_k}'\n",
    "                                filename = filename.replace('.','_')\n",
    "\n",
    "                                if filename in written_yaml_files:\n",
    "                                    continue\n",
    "                                # write yaml file\n",
    "                                data = {\n",
    "                                    'model': {\n",
    "                                        'model_type': 'lightgcn',\n",
    "                                        'embed_size': embed_size,\n",
    "                                        'n_layers': n_layers\n",
    "                                    },\n",
    "                                    'train': {\n",
    "                                        'batch_size': batch_size,\n",
    "                                        'decay': decay,\n",
    "                                        'epochs': epochs,\n",
    "                                        'learning_rate': learning_rate,\n",
    "                                        'eval_epoch': eval_epoch,\n",
    "                                        'top_k': top_k\n",
    "                                    },\n",
    "                                    'info': {\n",
    "                                        'save_model' : True, # whether to save model\n",
    "                                        'save_epoch' : 100, # if save_model is set to True, save the model every save_epoch\n",
    "                                        'metrics' : [\"recall\", \"ndcg\", \"precision\", 'map'], # metrics for evaluation\n",
    "                                        'MODEL_DIR' : './models/' # directory of saved models\n",
    "                                    }\n",
    "                                }\n",
    "\n",
    "                                with open(f'{yaml_dir}/{filename}.yaml', 'w') as outfile:\n",
    "                                    yaml.dump(data, outfile, default_flow_style=False)\n",
    "                                outfile.close()\n",
    "\n",
    "                                yaml_files.append(filename)\n",
    "\n",
    "yaml_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yudrew/Dev/cis5190/.conda/lib/python3.9/site-packages/recommenders/models/deeprec/DataModel/ImplicitCF.py:73: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = train if test is None else train.append(test)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already create adjacency matrix.\n",
      "Already normalize adjacency matrix.\n",
      "Using xavier initialization.\n",
      "Epoch 1 (train)7.8s: train loss = 0.48999 = (mf)0.47354 + (embed)0.01645\n",
      "Epoch 2 (train)6.6s: train loss = 0.24090 = (mf)0.18994 + (embed)0.05096\n",
      "Epoch 3 (train)6.3s: train loss = 0.19249 = (mf)0.13196 + (embed)0.06053\n",
      "Epoch 4 (train)6.4s: train loss = 0.17089 = (mf)0.10682 + (embed)0.06407\n",
      "Epoch 5 (train)6.2s + (eval)2.5s: train loss = 0.15921 = (mf)0.09360 + (embed)0.06561, recall = 0.01797, ndcg = 0.03956, precision = 0.03591, map = 0.00706\n",
      "Epoch 6 (train)6.1s: train loss = 0.15028 = (mf)0.08366 + (embed)0.06662\n",
      "Epoch 7 (train)6.2s: train loss = 0.14347 = (mf)0.07583 + (embed)0.06764\n",
      "Epoch 8 (train)6.2s: train loss = 0.13909 = (mf)0.07060 + (embed)0.06849\n",
      "Epoch 9 (train)6.2s: train loss = 0.13610 = (mf)0.06660 + (embed)0.06950\n",
      "Epoch 10 (train)6.5s + (eval)1.5s: train loss = 0.13179 = (mf)0.06142 + (embed)0.07037, recall = 0.01856, ndcg = 0.04135, precision = 0.03747, map = 0.00731\n",
      "Epoch 11 (train)6.3s: train loss = 0.13107 = (mf)0.06000 + (embed)0.07107\n",
      "Epoch 12 (train)6.2s: train loss = 0.12867 = (mf)0.05698 + (embed)0.07169\n",
      "Epoch 13 (train)6.2s: train loss = 0.12706 = (mf)0.05448 + (embed)0.07258\n",
      "Epoch 14 (train)6.1s: train loss = 0.12431 = (mf)0.05131 + (embed)0.07300\n",
      "Epoch 15 (train)6.1s + (eval)1.1s: train loss = 0.12390 = (mf)0.05042 + (embed)0.07348, recall = 0.01888, ndcg = 0.04308, precision = 0.03792, map = 0.00773\n",
      "Epoch 16 (train)6.1s: train loss = 0.12366 = (mf)0.04973 + (embed)0.07393\n",
      "Epoch 17 (train)6.0s: train loss = 0.12161 = (mf)0.04726 + (embed)0.07435\n",
      "Epoch 18 (train)6.0s: train loss = 0.12112 = (mf)0.04633 + (embed)0.07479\n",
      "Epoch 19 (train)6.0s: train loss = 0.12132 = (mf)0.04617 + (embed)0.07514\n",
      "Epoch 20 (train)6.1s + (eval)1.1s: train loss = 0.12021 = (mf)0.04475 + (embed)0.07546, recall = 0.01874, ndcg = 0.04349, precision = 0.03792, map = 0.00776\n",
      "Epoch 21 (train)6.0s: train loss = 0.11953 = (mf)0.04385 + (embed)0.07568\n",
      "Epoch 22 (train)6.2s: train loss = 0.11888 = (mf)0.04286 + (embed)0.07601\n",
      "Epoch 23 (train)6.4s: train loss = 0.11861 = (mf)0.04238 + (embed)0.07623\n",
      "Epoch 24 (train)6.0s: train loss = 0.11819 = (mf)0.04171 + (embed)0.07648\n",
      "Epoch 25 (train)6.2s + (eval)1.2s: train loss = 0.11840 = (mf)0.04179 + (embed)0.07662, recall = 0.01928, ndcg = 0.04311, precision = 0.03864, map = 0.00757\n",
      "Epoch 26 (train)6.1s: train loss = 0.11776 = (mf)0.04076 + (embed)0.07701\n",
      "Epoch 27 (train)6.1s: train loss = 0.11687 = (mf)0.03988 + (embed)0.07699\n",
      "Epoch 28 (train)6.1s: train loss = 0.11701 = (mf)0.03975 + (embed)0.07726\n",
      "Epoch 29 (train)6.0s: train loss = 0.11668 = (mf)0.03927 + (embed)0.07741\n",
      "Epoch 30 (train)6.0s + (eval)1.1s: train loss = 0.11607 = (mf)0.03860 + (embed)0.07747, recall = 0.01987, ndcg = 0.04415, precision = 0.04013, map = 0.00776\n",
      "Epoch 31 (train)6.1s: train loss = 0.11649 = (mf)0.03875 + (embed)0.07774\n",
      "Epoch 32 (train)6.0s: train loss = 0.11645 = (mf)0.03865 + (embed)0.07780\n",
      "Epoch 33 (train)6.1s: train loss = 0.11608 = (mf)0.03831 + (embed)0.07777\n",
      "Epoch 34 (train)6.2s: train loss = 0.11529 = (mf)0.03734 + (embed)0.07795\n",
      "Epoch 35 (train)6.1s + (eval)1.1s: train loss = 0.11560 = (mf)0.03762 + (embed)0.07798, recall = 0.01986, ndcg = 0.04534, precision = 0.04039, map = 0.00791\n",
      "Epoch 36 (train)6.0s: train loss = 0.11535 = (mf)0.03723 + (embed)0.07812\n",
      "Epoch 37 (train)6.2s: train loss = 0.11491 = (mf)0.03681 + (embed)0.07810\n",
      "Epoch 38 (train)6.0s: train loss = 0.11570 = (mf)0.03753 + (embed)0.07817\n",
      "Epoch 39 (train)6.1s: train loss = 0.11523 = (mf)0.03674 + (embed)0.07849\n",
      "Epoch 40 (train)6.5s + (eval)1.3s: train loss = 0.11572 = (mf)0.03727 + (embed)0.07845, recall = 0.02040, ndcg = 0.04645, precision = 0.04149, map = 0.00820\n",
      "Epoch 41 (train)6.5s: train loss = 0.11507 = (mf)0.03651 + (embed)0.07856\n",
      "Epoch 42 (train)6.3s: train loss = 0.11456 = (mf)0.03593 + (embed)0.07862\n",
      "Epoch 43 (train)6.1s: train loss = 0.11478 = (mf)0.03627 + (embed)0.07852\n",
      "Epoch 44 (train)6.1s: train loss = 0.11408 = (mf)0.03550 + (embed)0.07858\n",
      "Epoch 45 (train)6.2s + (eval)1.2s: train loss = 0.11459 = (mf)0.03587 + (embed)0.07872, recall = 0.02030, ndcg = 0.04538, precision = 0.04091, map = 0.00798\n",
      "Epoch 46 (train)6.1s: train loss = 0.11454 = (mf)0.03575 + (embed)0.07879\n",
      "Epoch 47 (train)6.1s: train loss = 0.11475 = (mf)0.03589 + (embed)0.07886\n",
      "Epoch 48 (train)6.1s: train loss = 0.11526 = (mf)0.03638 + (embed)0.07888\n",
      "Epoch 49 (train)6.0s: train loss = 0.11331 = (mf)0.03437 + (embed)0.07894\n",
      "Epoch 50 (train)6.0s + (eval)1.1s: train loss = 0.11364 = (mf)0.03476 + (embed)0.07888, recall = 0.01979, ndcg = 0.04539, precision = 0.04052, map = 0.00790\n",
      "Took 322.64040570900397 seconds for training.\n",
      "MAP:\t0.007902\n",
      "NDCG:\t0.045388\n",
      "Precision@K:\t0.040519\n",
      "Recall@K:\t0.019788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7r/4qkpzm5s7tn9h45nw1nf4qy00000gn/T/ipykernel_3031/3816403685.py:63: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  old_results_df = old_results_df.append(results_df)\n",
      "/Users/yudrew/Dev/cis5190/.conda/lib/python3.9/site-packages/recommenders/models/deeprec/DataModel/ImplicitCF.py:73: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = train if test is None else train.append(test)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already create adjacency matrix.\n",
      "Already normalize adjacency matrix.\n",
      "Using xavier initialization.\n",
      "Epoch 1 (train)6.9s: train loss = 0.48999 = (mf)0.47364 + (embed)0.01635\n",
      "Epoch 2 (train)6.0s: train loss = 0.23840 = (mf)0.18689 + (embed)0.05151\n",
      "Epoch 3 (train)6.1s: train loss = 0.19046 = (mf)0.12932 + (embed)0.06114\n",
      "Epoch 4 (train)6.1s: train loss = 0.16845 = (mf)0.10378 + (embed)0.06466\n",
      "Epoch 5 (train)6.0s + (eval)2.1s: train loss = 0.15732 = (mf)0.09108 + (embed)0.06624, recall = 0.01832, ndcg = 0.04011, precision = 0.03643, map = 0.00712\n",
      "Epoch 6 (train)6.1s: train loss = 0.14851 = (mf)0.08123 + (embed)0.06728\n",
      "Epoch 7 (train)6.1s: train loss = 0.14177 = (mf)0.07346 + (embed)0.06831\n",
      "Epoch 8 (train)6.1s: train loss = 0.13729 = (mf)0.06811 + (embed)0.06918\n",
      "Epoch 9 (train)6.2s: train loss = 0.13475 = (mf)0.06453 + (embed)0.07022\n",
      "Epoch 10 (train)6.1s + (eval)1.3s: train loss = 0.13069 = (mf)0.05963 + (embed)0.07106, recall = 0.01874, ndcg = 0.04209, precision = 0.03766, map = 0.00749\n",
      "Epoch 11 (train)6.1s: train loss = 0.13008 = (mf)0.05835 + (embed)0.07173\n",
      "Epoch 12 (train)6.0s: train loss = 0.12776 = (mf)0.05538 + (embed)0.07238\n",
      "Epoch 13 (train)6.0s: train loss = 0.12634 = (mf)0.05314 + (embed)0.07321\n",
      "Epoch 14 (train)6.1s: train loss = 0.12372 = (mf)0.05010 + (embed)0.07362\n",
      "Epoch 15 (train)6.0s + (eval)1.1s: train loss = 0.12341 = (mf)0.04926 + (embed)0.07415, recall = 0.01916, ndcg = 0.04353, precision = 0.03890, map = 0.00774\n",
      "Epoch 16 (train)6.0s: train loss = 0.12339 = (mf)0.04884 + (embed)0.07455\n",
      "Epoch 17 (train)6.0s: train loss = 0.12133 = (mf)0.04640 + (embed)0.07493\n",
      "Epoch 18 (train)6.1s: train loss = 0.12072 = (mf)0.04535 + (embed)0.07537\n",
      "Epoch 19 (train)6.0s: train loss = 0.12121 = (mf)0.04554 + (embed)0.07567\n",
      "Epoch 20 (train)6.1s + (eval)1.1s: train loss = 0.12021 = (mf)0.04425 + (embed)0.07596, recall = 0.02006, ndcg = 0.04473, precision = 0.03994, map = 0.00797\n",
      "Epoch 21 (train)6.0s: train loss = 0.11962 = (mf)0.04340 + (embed)0.07622\n",
      "Epoch 22 (train)6.3s: train loss = 0.11904 = (mf)0.04251 + (embed)0.07653\n",
      "Epoch 23 (train)6.2s: train loss = 0.11872 = (mf)0.04206 + (embed)0.07666\n",
      "Epoch 24 (train)6.1s: train loss = 0.11821 = (mf)0.04130 + (embed)0.07690\n",
      "Epoch 25 (train)6.2s + (eval)1.3s: train loss = 0.11832 = (mf)0.04132 + (embed)0.07700, recall = 0.02011, ndcg = 0.04465, precision = 0.04013, map = 0.00780\n",
      "Epoch 26 (train)6.2s: train loss = 0.11800 = (mf)0.04064 + (embed)0.07735\n",
      "Epoch 27 (train)6.2s: train loss = 0.11707 = (mf)0.03977 + (embed)0.07730\n",
      "Epoch 28 (train)6.1s: train loss = 0.11736 = (mf)0.03977 + (embed)0.07759\n",
      "Epoch 29 (train)6.1s: train loss = 0.11676 = (mf)0.03908 + (embed)0.07768\n",
      "Epoch 30 (train)6.2s + (eval)1.1s: train loss = 0.11643 = (mf)0.03870 + (embed)0.07773, recall = 0.02042, ndcg = 0.04516, precision = 0.04084, map = 0.00799\n",
      "Epoch 31 (train)6.2s: train loss = 0.11645 = (mf)0.03849 + (embed)0.07796\n",
      "Epoch 32 (train)6.4s: train loss = 0.11671 = (mf)0.03873 + (embed)0.07798\n",
      "Epoch 33 (train)6.2s: train loss = 0.11640 = (mf)0.03838 + (embed)0.07802\n",
      "Epoch 34 (train)6.3s: train loss = 0.11567 = (mf)0.03748 + (embed)0.07819\n",
      "Epoch 35 (train)6.2s + (eval)1.1s: train loss = 0.11598 = (mf)0.03773 + (embed)0.07825, recall = 0.02011, ndcg = 0.04641, precision = 0.04078, map = 0.00824\n",
      "Epoch 36 (train)6.3s: train loss = 0.11540 = (mf)0.03700 + (embed)0.07839\n",
      "Epoch 37 (train)6.1s: train loss = 0.11521 = (mf)0.03694 + (embed)0.07827\n",
      "Epoch 38 (train)6.2s: train loss = 0.11593 = (mf)0.03760 + (embed)0.07832\n",
      "Epoch 39 (train)6.1s: train loss = 0.11555 = (mf)0.03687 + (embed)0.07868\n",
      "Epoch 40 (train)6.3s + (eval)1.2s: train loss = 0.11594 = (mf)0.03735 + (embed)0.07860, recall = 0.02031, ndcg = 0.04640, precision = 0.04136, map = 0.00820\n",
      "Epoch 41 (train)6.1s: train loss = 0.11510 = (mf)0.03639 + (embed)0.07871\n",
      "Epoch 42 (train)6.4s: train loss = 0.11464 = (mf)0.03589 + (embed)0.07875\n",
      "Epoch 43 (train)6.0s: train loss = 0.11488 = (mf)0.03624 + (embed)0.07863\n",
      "Epoch 44 (train)6.0s: train loss = 0.11434 = (mf)0.03561 + (embed)0.07873\n",
      "Epoch 45 (train)6.3s + (eval)1.2s: train loss = 0.11485 = (mf)0.03595 + (embed)0.07890, recall = 0.02009, ndcg = 0.04584, precision = 0.04052, map = 0.00827\n",
      "Epoch 46 (train)6.3s: train loss = 0.11464 = (mf)0.03571 + (embed)0.07893\n",
      "Epoch 47 (train)6.1s: train loss = 0.11511 = (mf)0.03610 + (embed)0.07901\n",
      "Epoch 48 (train)6.2s: train loss = 0.11541 = (mf)0.03640 + (embed)0.07901\n",
      "Epoch 49 (train)6.4s: train loss = 0.11361 = (mf)0.03451 + (embed)0.07909\n",
      "Epoch 50 (train)6.4s + (eval)1.2s: train loss = 0.11375 = (mf)0.03477 + (embed)0.07898, recall = 0.01984, ndcg = 0.04593, precision = 0.04032, map = 0.00814\n",
      "Took 321.25011254200217 seconds for training.\n",
      "MAP:\t0.008144\n",
      "NDCG:\t0.045928\n",
      "Precision@K:\t0.040325\n",
      "Recall@K:\t0.019842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7r/4qkpzm5s7tn9h45nw1nf4qy00000gn/T/ipykernel_3031/3816403685.py:63: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  old_results_df = old_results_df.append(results_df)\n",
      "/Users/yudrew/Dev/cis5190/.conda/lib/python3.9/site-packages/recommenders/models/deeprec/DataModel/ImplicitCF.py:73: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = train if test is None else train.append(test)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already create adjacency matrix.\n",
      "Already normalize adjacency matrix.\n",
      "Using xavier initialization.\n",
      "Epoch 1 (train)7.3s: train loss = 0.60098 = (mf)0.52086 + (embed)0.08011\n",
      "Epoch 2 (train)6.1s: train loss = 0.51032 = (mf)0.29488 + (embed)0.21544\n",
      "Epoch 3 (train)6.1s: train loss = 0.49339 = (mf)0.25826 + (embed)0.23513\n",
      "Epoch 4 (train)6.2s: train loss = 0.48263 = (mf)0.24533 + (embed)0.23730\n",
      "Epoch 5 (train)6.1s + (eval)2.1s: train loss = 0.47746 = (mf)0.24218 + (embed)0.23528, recall = 0.01634, ndcg = 0.03600, precision = 0.03247, map = 0.00633\n",
      "Epoch 6 (train)6.1s: train loss = 0.47268 = (mf)0.23879 + (embed)0.23390\n",
      "Epoch 7 (train)6.1s: train loss = 0.46930 = (mf)0.23569 + (embed)0.23361\n",
      "Epoch 8 (train)6.1s: train loss = 0.46794 = (mf)0.23582 + (embed)0.23212\n",
      "Epoch 9 (train)6.1s: train loss = 0.46702 = (mf)0.23559 + (embed)0.23144\n",
      "Epoch 10 (train)6.1s + (eval)1.2s: train loss = 0.46541 = (mf)0.23372 + (embed)0.23170, recall = 0.01790, ndcg = 0.03894, precision = 0.03532, map = 0.00690\n",
      "Epoch 11 (train)6.1s: train loss = 0.46590 = (mf)0.23503 + (embed)0.23087\n",
      "Epoch 12 (train)6.1s: train loss = 0.46442 = (mf)0.23359 + (embed)0.23083\n",
      "Epoch 13 (train)6.0s: train loss = 0.46510 = (mf)0.23390 + (embed)0.23119\n",
      "Epoch 14 (train)6.1s: train loss = 0.46223 = (mf)0.23126 + (embed)0.23097\n",
      "Epoch 15 (train)6.1s + (eval)1.2s: train loss = 0.46260 = (mf)0.23131 + (embed)0.23129, recall = 0.01774, ndcg = 0.03940, precision = 0.03532, map = 0.00702\n",
      "Epoch 16 (train)6.1s: train loss = 0.46364 = (mf)0.23258 + (embed)0.23106\n",
      "Epoch 17 (train)6.1s: train loss = 0.46116 = (mf)0.22991 + (embed)0.23125\n",
      "Epoch 18 (train)6.1s: train loss = 0.46145 = (mf)0.22969 + (embed)0.23176\n",
      "Epoch 19 (train)6.1s: train loss = 0.46301 = (mf)0.23135 + (embed)0.23166\n",
      "Epoch 20 (train)6.1s + (eval)1.2s: train loss = 0.46095 = (mf)0.22917 + (embed)0.23178, recall = 0.01828, ndcg = 0.03982, precision = 0.03610, map = 0.00713\n",
      "Epoch 21 (train)6.1s: train loss = 0.46148 = (mf)0.22957 + (embed)0.23191\n",
      "Epoch 22 (train)6.1s: train loss = 0.45957 = (mf)0.22717 + (embed)0.23240\n",
      "Epoch 23 (train)6.0s: train loss = 0.46060 = (mf)0.22869 + (embed)0.23191\n",
      "Epoch 24 (train)6.1s: train loss = 0.46096 = (mf)0.22882 + (embed)0.23214\n",
      "Epoch 25 (train)6.1s + (eval)1.2s: train loss = 0.46086 = (mf)0.22922 + (embed)0.23164, recall = 0.01854, ndcg = 0.03997, precision = 0.03649, map = 0.00715\n",
      "Epoch 26 (train)6.1s: train loss = 0.46119 = (mf)0.22843 + (embed)0.23276\n",
      "Epoch 27 (train)6.0s: train loss = 0.46041 = (mf)0.22817 + (embed)0.23224\n",
      "Epoch 28 (train)6.1s: train loss = 0.46032 = (mf)0.22780 + (embed)0.23252\n",
      "Epoch 29 (train)6.1s: train loss = 0.46000 = (mf)0.22733 + (embed)0.23267\n",
      "Epoch 30 (train)6.1s + (eval)1.2s: train loss = 0.45943 = (mf)0.22679 + (embed)0.23263, recall = 0.01862, ndcg = 0.04050, precision = 0.03675, map = 0.00721\n",
      "Epoch 31 (train)6.1s: train loss = 0.46082 = (mf)0.22754 + (embed)0.23328\n",
      "Epoch 32 (train)6.1s: train loss = 0.46065 = (mf)0.22792 + (embed)0.23273\n",
      "Epoch 33 (train)6.1s: train loss = 0.46018 = (mf)0.22774 + (embed)0.23244\n",
      "Epoch 34 (train)6.2s: train loss = 0.45929 = (mf)0.22617 + (embed)0.23312\n",
      "Epoch 35 (train)6.1s + (eval)1.2s: train loss = 0.46045 = (mf)0.22767 + (embed)0.23278, recall = 0.01880, ndcg = 0.04072, precision = 0.03714, map = 0.00730\n",
      "Epoch 36 (train)6.3s: train loss = 0.45929 = (mf)0.22615 + (embed)0.23315\n",
      "Epoch 37 (train)6.3s: train loss = 0.45890 = (mf)0.22606 + (embed)0.23284\n",
      "Epoch 38 (train)6.1s: train loss = 0.46016 = (mf)0.22717 + (embed)0.23300\n",
      "Epoch 39 (train)6.1s: train loss = 0.46071 = (mf)0.22760 + (embed)0.23311\n",
      "Epoch 40 (train)6.1s + (eval)1.1s: train loss = 0.46066 = (mf)0.22813 + (embed)0.23253, recall = 0.01831, ndcg = 0.04001, precision = 0.03636, map = 0.00715\n",
      "Epoch 41 (train)6.2s: train loss = 0.46042 = (mf)0.22742 + (embed)0.23300\n",
      "Epoch 42 (train)6.1s: train loss = 0.46000 = (mf)0.22689 + (embed)0.23312\n",
      "Epoch 43 (train)6.1s: train loss = 0.45993 = (mf)0.22781 + (embed)0.23212\n",
      "Epoch 44 (train)6.1s: train loss = 0.45911 = (mf)0.22597 + (embed)0.23315\n",
      "Epoch 45 (train)6.1s + (eval)1.1s: train loss = 0.46076 = (mf)0.22738 + (embed)0.23338, recall = 0.01865, ndcg = 0.04094, precision = 0.03701, map = 0.00729\n",
      "Epoch 46 (train)6.1s: train loss = 0.46029 = (mf)0.22719 + (embed)0.23309\n",
      "Epoch 47 (train)6.1s: train loss = 0.46080 = (mf)0.22742 + (embed)0.23338\n",
      "Epoch 48 (train)6.1s: train loss = 0.46069 = (mf)0.22782 + (embed)0.23286\n",
      "Epoch 49 (train)6.2s: train loss = 0.45919 = (mf)0.22579 + (embed)0.23340\n",
      "Epoch 50 (train)6.1s + (eval)1.2s: train loss = 0.45838 = (mf)0.22471 + (embed)0.23366, recall = 0.01872, ndcg = 0.04051, precision = 0.03701, map = 0.00720\n",
      "Took 319.1091930839975 seconds for training.\n",
      "MAP:\t0.007201\n",
      "NDCG:\t0.040509\n",
      "Precision@K:\t0.037013\n",
      "Recall@K:\t0.018720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7r/4qkpzm5s7tn9h45nw1nf4qy00000gn/T/ipykernel_3031/3816403685.py:63: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  old_results_df = old_results_df.append(results_df)\n",
      "/Users/yudrew/Dev/cis5190/.conda/lib/python3.9/site-packages/recommenders/models/deeprec/DataModel/ImplicitCF.py:73: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = train if test is None else train.append(test)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already create adjacency matrix.\n",
      "Already normalize adjacency matrix.\n",
      "Using xavier initialization.\n",
      "Epoch 1 (train)7.0s: train loss = 0.60126 = (mf)0.52111 + (embed)0.08015\n",
      "Epoch 2 (train)6.1s: train loss = 0.51106 = (mf)0.29617 + (embed)0.21489\n",
      "Epoch 3 (train)6.1s: train loss = 0.49395 = (mf)0.25913 + (embed)0.23482\n",
      "Epoch 4 (train)6.1s: train loss = 0.48305 = (mf)0.24575 + (embed)0.23729\n",
      "Epoch 5 (train)6.1s + (eval)2.2s: train loss = 0.47761 = (mf)0.24237 + (embed)0.23523, recall = 0.01672, ndcg = 0.03690, precision = 0.03325, map = 0.00649\n",
      "Epoch 6 (train)6.1s: train loss = 0.47309 = (mf)0.23915 + (embed)0.23394\n",
      "Epoch 7 (train)6.1s: train loss = 0.46947 = (mf)0.23588 + (embed)0.23359\n",
      "Epoch 8 (train)6.1s: train loss = 0.46804 = (mf)0.23588 + (embed)0.23216\n",
      "Epoch 9 (train)6.1s: train loss = 0.46705 = (mf)0.23560 + (embed)0.23145\n",
      "Epoch 10 (train)6.1s + (eval)1.2s: train loss = 0.46555 = (mf)0.23385 + (embed)0.23170, recall = 0.01755, ndcg = 0.03903, precision = 0.03494, map = 0.00697\n",
      "Epoch 11 (train)6.1s: train loss = 0.46616 = (mf)0.23532 + (embed)0.23085\n",
      "Epoch 12 (train)6.1s: train loss = 0.46441 = (mf)0.23365 + (embed)0.23076\n",
      "Epoch 13 (train)6.1s: train loss = 0.46513 = (mf)0.23397 + (embed)0.23116\n",
      "Epoch 14 (train)6.0s: train loss = 0.46227 = (mf)0.23129 + (embed)0.23097\n",
      "Epoch 15 (train)6.1s + (eval)1.2s: train loss = 0.46261 = (mf)0.23131 + (embed)0.23130, recall = 0.01765, ndcg = 0.03961, precision = 0.03519, map = 0.00709\n",
      "Epoch 16 (train)6.1s: train loss = 0.46365 = (mf)0.23256 + (embed)0.23109\n",
      "Epoch 17 (train)6.1s: train loss = 0.46127 = (mf)0.22999 + (embed)0.23128\n",
      "Epoch 18 (train)6.0s: train loss = 0.46154 = (mf)0.22980 + (embed)0.23174\n",
      "Epoch 19 (train)6.1s: train loss = 0.46310 = (mf)0.23149 + (embed)0.23161\n",
      "Epoch 20 (train)6.1s + (eval)1.2s: train loss = 0.46095 = (mf)0.22915 + (embed)0.23181, recall = 0.01794, ndcg = 0.03995, precision = 0.03571, map = 0.00721\n",
      "Epoch 21 (train)6.0s: train loss = 0.46148 = (mf)0.22959 + (embed)0.23189\n",
      "Epoch 22 (train)6.1s: train loss = 0.45977 = (mf)0.22736 + (embed)0.23241\n",
      "Epoch 23 (train)6.2s: train loss = 0.46061 = (mf)0.22871 + (embed)0.23190\n",
      "Epoch 24 (train)6.1s: train loss = 0.46097 = (mf)0.22882 + (embed)0.23215\n",
      "Epoch 25 (train)6.1s + (eval)1.1s: train loss = 0.46090 = (mf)0.22931 + (embed)0.23159, recall = 0.01848, ndcg = 0.04022, precision = 0.03675, map = 0.00719\n",
      "Epoch 26 (train)6.1s: train loss = 0.46109 = (mf)0.22836 + (embed)0.23273\n",
      "Epoch 27 (train)6.1s: train loss = 0.46041 = (mf)0.22818 + (embed)0.23223\n",
      "Epoch 28 (train)6.1s: train loss = 0.46039 = (mf)0.22783 + (embed)0.23256\n",
      "Epoch 29 (train)6.0s: train loss = 0.45999 = (mf)0.22730 + (embed)0.23269\n",
      "Epoch 30 (train)6.0s + (eval)1.1s: train loss = 0.45956 = (mf)0.22695 + (embed)0.23261, recall = 0.01861, ndcg = 0.04052, precision = 0.03675, map = 0.00727\n",
      "Epoch 31 (train)6.1s: train loss = 0.46076 = (mf)0.22753 + (embed)0.23322\n",
      "Epoch 32 (train)6.1s: train loss = 0.46070 = (mf)0.22800 + (embed)0.23270\n",
      "Epoch 33 (train)6.1s: train loss = 0.46014 = (mf)0.22778 + (embed)0.23236\n",
      "Epoch 34 (train)6.1s: train loss = 0.45922 = (mf)0.22612 + (embed)0.23310\n",
      "Epoch 35 (train)6.0s + (eval)1.1s: train loss = 0.46051 = (mf)0.22772 + (embed)0.23279, recall = 0.01858, ndcg = 0.04064, precision = 0.03675, map = 0.00732\n",
      "Epoch 36 (train)6.1s: train loss = 0.45933 = (mf)0.22617 + (embed)0.23316\n",
      "Epoch 37 (train)6.1s: train loss = 0.45895 = (mf)0.22607 + (embed)0.23288\n",
      "Epoch 38 (train)6.2s: train loss = 0.46018 = (mf)0.22719 + (embed)0.23299\n",
      "Epoch 39 (train)6.1s: train loss = 0.46076 = (mf)0.22769 + (embed)0.23307\n",
      "Epoch 40 (train)6.1s + (eval)1.2s: train loss = 0.46069 = (mf)0.22820 + (embed)0.23249, recall = 0.01775, ndcg = 0.03948, precision = 0.03565, map = 0.00704\n",
      "Epoch 41 (train)6.0s: train loss = 0.46048 = (mf)0.22752 + (embed)0.23296\n",
      "Epoch 42 (train)6.1s: train loss = 0.46001 = (mf)0.22696 + (embed)0.23305\n",
      "Epoch 43 (train)6.3s: train loss = 0.45995 = (mf)0.22788 + (embed)0.23207\n",
      "Epoch 44 (train)6.6s: train loss = 0.45910 = (mf)0.22599 + (embed)0.23312\n",
      "Epoch 45 (train)6.2s + (eval)1.2s: train loss = 0.46082 = (mf)0.22750 + (embed)0.23332, recall = 0.01843, ndcg = 0.04057, precision = 0.03662, map = 0.00728\n",
      "Epoch 46 (train)6.3s: train loss = 0.46031 = (mf)0.22728 + (embed)0.23303\n",
      "Epoch 47 (train)6.1s: train loss = 0.46090 = (mf)0.22757 + (embed)0.23333\n",
      "Epoch 48 (train)6.1s: train loss = 0.46072 = (mf)0.22790 + (embed)0.23282\n",
      "Epoch 49 (train)6.1s: train loss = 0.45922 = (mf)0.22588 + (embed)0.23334\n",
      "Epoch 50 (train)6.1s + (eval)1.3s: train loss = 0.45829 = (mf)0.22469 + (embed)0.23361, recall = 0.01855, ndcg = 0.04030, precision = 0.03675, map = 0.00717\n",
      "Took 319.3834074999977 seconds for training.\n",
      "MAP:\t0.007170\n",
      "NDCG:\t0.040302\n",
      "Precision@K:\t0.036753\n",
      "Recall@K:\t0.018551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7r/4qkpzm5s7tn9h45nw1nf4qy00000gn/T/ipykernel_3031/3816403685.py:63: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  old_results_df = old_results_df.append(results_df)\n"
     ]
    }
   ],
   "source": [
    "# top k items to recommend\n",
    "TOP_K = 10\n",
    "\n",
    "# Select MovieLens data size: 100k, 1m, 10m, or 20m\n",
    "AMAZON_DATA_SIZE = len(subset_df)\n",
    "\n",
    "# Model parameters\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "SEED = DEFAULT_SEED  # Set None for non-deterministic results\n",
    "\n",
    "results = {}\n",
    "\n",
    "for filename in yaml_files: \n",
    "    yaml_file = f'yamls/{filename}.yaml'\n",
    "    user_file = f\"embeddings/{filename}_user_embeddings.csv\"\n",
    "    item_file = f\"embeddings/{filename}_item_embeddings.csv\"\n",
    "    df = subset_df[:AMAZON_DATA_SIZE]\n",
    "    df = df.rename(columns={'userId': 'userID', 'productId': 'itemID'})\n",
    "    df.head()\n",
    "    train, test = python_stratified_split(df, ratio=0.75)\n",
    "    data = ImplicitCF(train=train, test=test, seed=SEED)\n",
    "    hparams = prepare_hparams(yaml_file,\n",
    "                            n_layers=3,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            epochs=EPOCHS,\n",
    "                            learning_rate=0.005,\n",
    "                            eval_epoch=5,\n",
    "                            top_k=TOP_K,\n",
    "                            )\n",
    "    model = LightGCN(hparams, data, seed=SEED)\n",
    "    with Timer() as train_time:\n",
    "        model.fit()\n",
    "\n",
    "    print(\"Took {} seconds for training.\".format(train_time.interval))\n",
    "    topk_scores = model.recommend_k_items(test, top_k=TOP_K, remove_seen=True)\n",
    "\n",
    "    topk_scores.head()\n",
    "    eval_map = map_at_k(test, topk_scores, k=TOP_K)\n",
    "    eval_ndcg = ndcg_at_k(test, topk_scores, k=TOP_K)\n",
    "    eval_precision = precision_at_k(test, topk_scores, k=TOP_K)\n",
    "    eval_recall = recall_at_k(test, topk_scores, k=TOP_K)\n",
    "\n",
    "    print(\"MAP:\\t%f\" % eval_map,\n",
    "        \"NDCG:\\t%f\" % eval_ndcg,\n",
    "        \"Precision@K:\\t%f\" % eval_precision,\n",
    "        \"Recall@K:\\t%f\" % eval_recall, sep='\\n')\n",
    "\n",
    "    results[filename] = [eval_map, eval_ndcg, eval_precision, eval_recall, train_time.interval]\n",
    "    model.infer_embedding(user_file, item_file)\n",
    "    save_and_compress_embeddings(user_file, \"userID\")\n",
    "    save_and_compress_embeddings(item_file, \"itemID\")\n",
    "\n",
    "    # write results each iteration in case things crash\n",
    "    results_df = pd.DataFrame.from_dict(results, orient='index', columns=['MAP', 'NDCG', 'Precision', 'Recall', 'Train Time'])\n",
    "    results_df['embed_size'], results_df['n_layers'], results_df['batch_size'], results_df['decay'], results_df['epochs'], results_df['learning_rate'], results_df['eval_epoch'], results_df['top_k'] = zip(*results_df.index.map(get_hyperparameters))\n",
    "    results_df.reset_index(inplace=True)\n",
    "    results_df.rename(columns={'index': 'yaml_file'}, inplace=True)\n",
    "\n",
    "    # read in previous results and append new results\n",
    "    old_results_df = pd.read_csv('lightgcn_results.csv')\n",
    "    old_results_df = old_results_df.append(results_df)\n",
    "    old_results_df.reset_index(inplace=True)\n",
    "    old_results_df.drop(columns=['index'], inplace=True)\n",
    "    drop_duplicates(old_results_df)\n",
    "    old_results_df.to_csv('lightgcn_results.csv', index=False)\n",
    "\n",
    "# pass results dict into a dataframe\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
